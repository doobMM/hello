{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVGaJM44MenpWRiCdvgPMC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doobMM/hello/blob/master/coteDB_TF_24_07_WK4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cost = tf.reduce_mean(tf.square(hypothesis - y_data))"
      ],
      "metadata": {
        "id": "Lpg4GaEkU--4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "#tf.enable_eager_execution()\n",
        "\n",
        "# Data\n",
        "x_data = [1, 2, 3, 4, 5]\n",
        "y_data = [1, 2, 3, 4, 5]\n",
        "\n",
        "# W, b initialize\n",
        "W = tf.Variable(2.9)\n",
        "b = tf.Variable(0.5)\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "for i in range(100+1): # W, b update\n",
        " # Gradient descent\n",
        "  with tf.GradientTape() as tape:\n",
        "    hypothesis = W * x_data + b\n",
        "    cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
        "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
        "    W.assign_sub(learning_rate * W_grad)\n",
        "    b.assign_sub(learning_rate * b_grad)\n",
        "    if i % 10 == 0:\n",
        "      print(\"{:5}|{:10.4f}|{:10.4}|{:10.6f}\".format(i, W.numpy(), b.numpy(), cost))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NNyRmCTWvJf",
        "outputId": "b4f80501-910c-4d0f-ae84-10432ccd23d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    0|    2.4520|     0.376| 45.660004\n",
            "   10|    1.1036|  0.003398|  0.206336\n",
            "   20|    1.0128|  -0.02091|  0.001026\n",
            "   30|    1.0065|  -0.02184|  0.000093\n",
            "   40|    1.0059|  -0.02123|  0.000083\n",
            "   50|    1.0057|  -0.02053|  0.000077\n",
            "   60|    1.0055|  -0.01984|  0.000072\n",
            "   70|    1.0053|  -0.01918|  0.000067\n",
            "   80|    1.0051|  -0.01854|  0.000063\n",
            "   90|    1.0050|  -0.01793|  0.000059\n",
            "  100|    1.0048|  -0.01733|  0.000055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" cost function in pure python \"\"\"\n",
        "\n",
        "import numpy as np\n",
        "X = np.array([1, 2, 3])\n",
        "Y = np.array([1, 2, 3])\n",
        "\n",
        "def cost_func(W, X, Y):\n",
        " c = 0\n",
        " for i in range(len(X)):\n",
        "   c += (W * X[i] - Y[i]) ** 2\n",
        " return c / len(X)\n",
        "\n",
        "for feed_W in np.linspace(-3, 5, num=15):\n",
        " curr_cost = cost_func(feed_W, X, Y)\n",
        " print(\"{:6.3f} | {:10.5f}\".format(feed_W, curr_cost))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDp7xYs-Y_4g",
        "outputId": "57c39431-83f0-4360-8748-fa54d1a99340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-3.000 |   74.66667\n",
            "-2.429 |   54.85714\n",
            "-1.857 |   38.09524\n",
            "-1.286 |   24.38095\n",
            "-0.714 |   13.71429\n",
            "-0.143 |    6.09524\n",
            " 0.429 |    1.52381\n",
            " 1.000 |    0.00000\n",
            " 1.571 |    1.52381\n",
            " 2.143 |    6.09524\n",
            " 2.714 |   13.71429\n",
            " 3.286 |   24.38095\n",
            " 3.857 |   38.09524\n",
            " 4.429 |   54.85714\n",
            " 5.000 |   74.66667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" cost fnc in tf \"\"\"\n",
        "X = np.array([1, 2, 3])\n",
        "Y = np.array([1, 2, 3])\n",
        "\n",
        "def cost_func(W, X, Y):\n",
        " hypothesis = X * W\n",
        " return tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "\n",
        "W_values = np.linspace(-3, 5, num=15)\n",
        "cost_values = []\n",
        "\n",
        "for feed_W in W_values:\n",
        " curr_cost = cost_func(feed_W, X, Y)\n",
        " cost_values.append(curr_cost)\n",
        " print(\"{:6.3f} | {:10.5f}\".format(feed_W, curr_cost))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeerzuv6ZPy4",
        "outputId": "1bd5aa51-7aa1-4a92-a753-c66a0a7e7b01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-3.000 |   74.66667\n",
            "-2.429 |   54.85714\n",
            "-1.857 |   38.09524\n",
            "-1.286 |   24.38095\n",
            "-0.714 |   13.71429\n",
            "-0.143 |    6.09524\n",
            " 0.429 |    1.52381\n",
            " 1.000 |    0.00000\n",
            " 1.571 |    1.52381\n",
            " 2.143 |    6.09524\n",
            " 2.714 |   13.71429\n",
            " 3.286 |   24.38095\n",
            " 3.857 |   38.09524\n",
            " 4.429 |   54.85714\n",
            " 5.000 |   74.66667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tf.set_random_seed(0) # for reproducibility\n",
        "\n",
        "x_data = [1., 2., 3., 4.]\n",
        "y_data = [1., 3., 5., 7.]\n",
        "\n",
        "#W = tf.Variable(tf.random_normal([1], -100., 100.))\n",
        "W = tf.Variable([5.0])\n",
        "\n",
        "for step in range(300):\n",
        "  hypothesis = W * X\n",
        "  cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "  alpha = 0.01\n",
        "  gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X))\n",
        "  descent = W - tf.multiply(alpha, gradient)\n",
        "  W.assign(descent)\n",
        "\n",
        "  if step % 10 == 0:\n",
        "    print('{:5} | {:10.4f} | {:10.6f}'.format(step, cost.numpy(), W.numpy()[0]))\n",
        "    #print(W.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcHUjOD9acjP",
        "outputId": "0b799193-369d-4001-f2e4-e51b359e1db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    0 |    74.6667 |   4.813334\n",
            "   10 |    28.7093 |   3.364572\n",
            "   20 |    11.0387 |   2.466224\n",
            "   30 |     4.2444 |   1.909177\n",
            "   40 |     1.6320 |   1.563762\n",
            "   50 |     0.6275 |   1.349578\n",
            "   60 |     0.2413 |   1.216766\n",
            "   70 |     0.0928 |   1.134412\n",
            "   80 |     0.0357 |   1.083346\n",
            "   90 |     0.0137 |   1.051681\n",
            "  100 |     0.0053 |   1.032047\n",
            "  110 |     0.0020 |   1.019871\n",
            "  120 |     0.0008 |   1.012322\n",
            "  130 |     0.0003 |   1.007641\n",
            "  140 |     0.0001 |   1.004738\n",
            "  150 |     0.0000 |   1.002938\n",
            "  160 |     0.0000 |   1.001822\n",
            "  170 |     0.0000 |   1.001130\n",
            "  180 |     0.0000 |   1.000700\n",
            "  190 |     0.0000 |   1.000434\n",
            "  200 |     0.0000 |   1.000269\n",
            "  210 |     0.0000 |   1.000167\n",
            "  220 |     0.0000 |   1.000103\n",
            "  230 |     0.0000 |   1.000064\n",
            "  240 |     0.0000 |   1.000040\n",
            "  250 |     0.0000 |   1.000025\n",
            "  260 |     0.0000 |   1.000015\n",
            "  270 |     0.0000 |   1.000009\n",
            "  280 |     0.0000 |   1.000006\n",
            "  290 |     0.0000 |   1.000004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data and label\n",
        "x1 = [ 73., 93., 89., 96., 73.]\n",
        "x2 = [ 80., 88., 91., 98., 66.]\n",
        "x3 = [ 75., 93., 90., 100., 70.]\n",
        "Y = [152., 185., 180., 196., 142.]\n",
        "\n",
        "\"\"\"\n",
        "# random weights\n",
        "w1 = tf.Variable(tf.random_normal([1]))\n",
        "w2 = tf.Variable(tf.random_normal([1]))\n",
        "w3 = tf.Variable(tf.random_normal([1]))\n",
        "b = tf.Variable(tf.random_normal([1]))\n",
        "\"\"\"\n",
        "w1 = tf.Variable(10.)\n",
        "w2 = tf.Variable(10.)\n",
        "w3 = tf.Variable(10.)\n",
        "b = tf.Variable(10.)\n",
        "\n",
        "learning_rate = 0.000001\n",
        "\n",
        "for i in range(1000+1):\n",
        "  # tf.GradientTape() to record the gradient of the cost function\n",
        "  with tf.GradientTape() as tape:\n",
        "    hypothesis = w1 * x1 + w2 * x2 + w3 * x3 + b\n",
        "    cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "\n",
        "    # calculates the gradients of the cost\n",
        "    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n",
        "\n",
        "    # update w1,w2,w3 and b\n",
        "    w1.assign_sub(learning_rate * w1_grad)\n",
        "    w2.assign_sub(learning_rate * w2_grad)\n",
        "    w3.assign_sub(learning_rate * w3_grad)\n",
        "    b.assign_sub(learning_rate * b_grad)\n",
        "    if i % 50 == 0:\n",
        "      print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFxb6LC5cU0A",
        "outputId": "284c1a08-8f23-484d-8fe6-71d1fb620299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    0 | 5793889.5000\n",
            "   50 |   64291.1562\n",
            "  100 |     715.2903\n",
            "  150 |       9.8461\n",
            "  200 |       2.0152\n",
            "  250 |       1.9252\n",
            "  300 |       1.9210\n",
            "  350 |       1.9177\n",
            "  400 |       1.9145\n",
            "  450 |       1.9114\n",
            "  500 |       1.9081\n",
            "  550 |       1.9050\n",
            "  600 |       1.9018\n",
            "  650 |       1.8986\n",
            "  700 |       1.8955\n",
            "  750 |       1.8923\n",
            "  800 |       1.8892\n",
            "  850 |       1.8861\n",
            "  900 |       1.8829\n",
            "  950 |       1.8798\n",
            " 1000 |       1.8767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.array([\n",
        " # X1, X2, X3, y\n",
        " [ 73., 80., 75., 152. ],\n",
        " [ 93., 88., 93., 185. ],\n",
        " [ 89., 91., 90., 180. ],\n",
        " [ 96., 98., 100., 196. ],\n",
        " [ 73., 66., 70., 142. ]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# slice data\n",
        "X = data[:, :-1]\n",
        "y = data[:, [-1]]\n",
        "\n",
        "W = tf.Variable(tf.random.normal([3, 1]))\n",
        "b = tf.Variable(tf.random.normal([1]))\n",
        "\n",
        "learning_rate = 0.000001\n",
        "\n",
        "# hypothesis, prediction function\n",
        "def predict(X):\n",
        " return tf.matmul(X, W) + b\n",
        "\n",
        "n_epochs = 2000\n",
        "for i in range(n_epochs+1):\n",
        " # record the gradient of the cost function\n",
        "\n",
        " with tf.GradientTape() as tape:\n",
        "  cost = tf.reduce_mean((tf.square(predict(X) - y)))\n",
        "\n",
        "  # calculates the gradients of the loss\n",
        "  W_grad, b_grad = tape.gradient(cost, [W, b])\n",
        "\n",
        "  # updates parameters (W and b)\n",
        "  W.assign_sub(learning_rate * W_grad)\n",
        "  b.assign_sub(learning_rate * b_grad)\n",
        "\n",
        "  if i % 100 == 0:\n",
        "    print(\"{:5} | {:10.4f}\".format(i, cost.numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5AY034hdbPX",
        "outputId": "0ad3491e-7f66-499b-bbc1-486bbe489f21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    0 | 47243.8945\n",
            "  100 |     6.4655\n",
            "  200 |     0.6485\n",
            "  300 |     0.6467\n",
            "  400 |     0.6456\n",
            "  500 |     0.6446\n",
            "  600 |     0.6436\n",
            "  700 |     0.6425\n",
            "  800 |     0.6415\n",
            "  900 |     0.6405\n",
            " 1000 |     0.6395\n",
            " 1100 |     0.6384\n",
            " 1200 |     0.6374\n",
            " 1300 |     0.6364\n",
            " 1400 |     0.6354\n",
            " 1500 |     0.6345\n",
            " 1600 |     0.6335\n",
            " 1700 |     0.6325\n",
            " 1800 |     0.6315\n",
            " 1900 |     0.6305\n",
            " 2000 |     0.6296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lab-05-2 로지스틱 회귀\n",
        "\n",
        "import tensorflow.contrib.eager as tfe # No module named 'tensorflow.contrib'\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))\n",
        "W = tf.Variable(tf.zeros([2,1]), name='weight')\n",
        "b = tf.Variable(tf.zeros([1]), name='bias')\n",
        "\n",
        "def logistic_regression(features):\n",
        " hypothesis = tf.div(1., 1. + tf.exp(tf.matmul(features, W) + b))\n",
        " return hypothesis\n",
        "\n",
        "def loss_fn(hypothesis, labels):\n",
        " cost = -tf.reduce_mean(labels * tf.log(loss_fn(hypothesis) + (1 - labels) * tf.log(1 - hypothesis)))\n",
        " return cost\n",
        "\n",
        "def grad(hypothesis, features, labels):\n",
        " with tf.GradientTape() as tape:\n",
        "   loss_value = loss_fn(hypothesis,labels)\n",
        " return tape.gradient(loss_value, [W,b])\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "for step in range(EPOCHS):\n",
        " for features, labels in tfe.Iterator(dataset):\n",
        "  grads = grad(logistic_regression(features), features, labels)\n",
        "  optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))\n",
        "  if step % 100 == 0:\n",
        "    print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(logistic_regression(features) ,labels)))\n",
        "\n",
        "def accuracy_fn(hypothesis, labels):\n",
        " predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        " accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))\n",
        " return accuracy\n",
        "\n",
        "test_acc = accuracy_fn(logistic_regression(x_test),y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "RQ5E_386ieLu",
        "outputId": "105d3853-dbef-421d-fe95-3b80c66ad7fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow.contrib'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-195fd4408cfd>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Lab-05-2 로지스틱 회귀\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "240724\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Medmh9P4mWmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(777)  # for reproducibility\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmYJRplNmLWl",
        "outputId": "66f490eb-b440-4bdb-fee8-5ef1fdb3f7a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* x_data가 2차원 배열이기에 2차원 공간에 표현하여 x1과 x2를 기준으로 y_data 0과 1로 구분하는 예제입니다\n",
        "* Logistic Classification 통해 보라색과 노란색 y_data(Label)을 구분해 보겠습니다.\n",
        "* Test 데이터는 붉은색의 위치와 같이 추론시 1의 값을 가지게 됩니다."
      ],
      "metadata": {
        "id": "Oc5Su_Ppm_Pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = [[1., 2.],\n",
        "          [2., 3.],\n",
        "          [3., 1.],\n",
        "          [4., 3.],\n",
        "          [5., 3.],\n",
        "          [6., 2.]]\n",
        "y_train = [[0.],\n",
        "          [0.],\n",
        "          [0.],\n",
        "          [1.],\n",
        "          [1.],\n",
        "          [1.]]\n",
        "\n",
        "x_test = [[5.,2.]]\n",
        "y_test = [[1.]]\n",
        "\n",
        "\n",
        "x1 = [x[0] for x in x_train]\n",
        "x2 = [x[1] for x in x_train]\n",
        "\n",
        "colors = [int(y[0] % 2) for y in y_train]\n",
        "plt.scatter(x1,x2, c=colors , marker='^')\n",
        "plt.scatter(x_test[0][0],x_test[0][1], c=\"red\")\n",
        "\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "RMOkLJrqmZKz",
        "outputId": "0b7e2502-1677-4af9-a7e9-29cb14812cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))#.repeat()"
      ],
      "metadata": {
        "id": "jkrIgQyAnNQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* W와 b은 학습을 통해 생성되는 모델에 쓰이는 Weight와 Bias (초기값을 variable : 0이나 Random값으로 가능 tf.random_normal([2, 1]) )"
      ],
      "metadata": {
        "id": "pXdZ30zJnT2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = tf.Variable(tf.zeros([2,1]), name='weight')\n",
        "b = tf.Variable(tf.zeros([1]), name='bias')"
      ],
      "metadata": {
        "id": "eHsOntYYnYLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* sigmoid라고도 부르고 logistic이라고도 부른다, 아래 함수는. 0과 1사이로 압축하는 데 사용한다."
      ],
      "metadata": {
        "id": "24JM955jpjRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression(features):\n",
        "    hypothesis  = tf.divide(1., 1. + tf.exp(tf.matmul(features, W) + b))\n",
        "    return hypothesis"
      ],
      "metadata": {
        "id": "a9MGZW3_nggW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(hypothesis, labels):\n",
        "    cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
        "    return cost\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)"
      ],
      "metadata": {
        "id": "w8mQfsa_nraq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Sigmoid 함수를 통해 예측값이 0.5보다 크면 1을 반환하고 0.5보다 작으면 0으로 반환합니다.\n",
        "* 가설을 통해 실재 값과 비교한 정확도를 측정합니다"
      ],
      "metadata": {
        "id": "ulb0KYgjn8vW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_fn(hypothesis, labels):\n",
        "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "pjcmpO_XoCb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* GradientTape를 통해 경사값을 계산합니다."
      ],
      "metadata": {
        "id": "w3Q1EkFNokDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad(features, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_value = loss_fn(logistic_regression(features),labels)\n",
        "    return tape.gradient(loss_value, [W,b])"
      ],
      "metadata": {
        "id": "A_DXEeD3oliS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 위의 Data를 Cost함수를 통해 학습시킨 후 모델을 생성합니다.\n",
        "* 새로운 Data를 통한 검증 수행 [5,2]의 Data로 테스트 수행 (그래프상 1이 나와야 정상입니다)"
      ],
      "metadata": {
        "id": "LFgJeHmvowgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 1001\n",
        "\n",
        "for step in range(EPOCHS):\n",
        "    for features, labels  in iter(dataset):\n",
        "        grads = grad(features, labels)\n",
        "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))\n",
        "        if step % 100 == 0:\n",
        "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(logistic_regression(features), labels)))\n",
        "test_acc = accuracy_fn(logistic_regression(x_test),y_test)\n",
        "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3xW-aagov7b",
        "outputId": "d1f5f507-0275-4f78-9002-1194a801e2cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 0, Loss: 0.6821\n",
            "Iter: 100, Loss: 0.5770\n",
            "Iter: 200, Loss: 0.5345\n",
            "Iter: 300, Loss: 0.5052\n",
            "Iter: 400, Loss: 0.4836\n",
            "Iter: 500, Loss: 0.4670\n",
            "Iter: 600, Loss: 0.4534\n",
            "Iter: 700, Loss: 0.4419\n",
            "Iter: 800, Loss: 0.4318\n",
            "Iter: 900, Loss: 0.4227\n",
            "Iter: 1000, Loss: 0.4143\n",
            "Testset Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# * Lab 06-1\n",
        "- 복습 필요, 원리는 조금 이해했지만 세부적인 표현형에 익숙치 않음"
      ],
      "metadata": {
        "id": "ksM_S9xWuIEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = [[1, 2, 1, 1],\n",
        " [2, 1, 3, 2],\n",
        " [3, 1, 3, 4],\n",
        " [4, 1, 5, 5],\n",
        " [1, 7, 5, 5],\n",
        " [1, 2, 5, 6],\n",
        " [1, 6, 6, 6],\n",
        " [1, 7, 7, 7]]\n",
        "y_data = [[0, 0, 1],\n",
        " [0, 0, 1],\n",
        " [0, 0, 1],\n",
        " [0, 1, 0],\n",
        " [0, 1, 0],\n",
        " [0, 1, 0],\n",
        " [1, 0, 0],\n",
        " [1, 0, 0]]"
      ],
      "metadata": {
        "id": "j0jgI6fLuLhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert into numpy and float format\n",
        "x_data = np.asarray(x_data, dtype=np.float32)\n",
        "y_data = np.asarray(y_data, dtype=np.float32)\n",
        "nb_classes = 3 #num classes"
      ],
      "metadata": {
        "id": "JPJnLsiauWhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Weight and bias setting\n",
        "W = tf.Variable(tf.random.normal([4, nb_classes]), name='weight')\n",
        "b = tf.Variable(tf.random.normal([nb_classes]), name='bias')\n",
        "variables = [W, b]"
      ],
      "metadata": {
        "id": "95mP-aXsugMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(variables)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CI4oY4Whu4Pm",
        "outputId": "1ffc4211-4a1c-40cd-d3c2-f36402024d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<tf.Variable 'weight:0' shape=(4, 3) dtype=float32, numpy=\n",
            "array([[-2.7697366e-01, -2.3162541e+00,  1.4861290e+00],\n",
            "       [-1.1312336e+00, -1.1059969e-01,  1.1424177e+00],\n",
            "       [-7.9997230e-01, -2.2132940e+00,  1.0443608e+00],\n",
            "       [ 1.5469295e+00, -6.0123701e-02,  6.6931121e-04]], dtype=float32)>, <tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([0.9083067, 1.7801584, 0.2447134], dtype=float32)>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax onehot test\n",
        "sample_db = [[8,2,1,4]]\n",
        "sample_db = np.asarray(sample_db, dtype=np.float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_Dagv7tvMue",
        "outputId": "d1b61fd3-9ba9-4ee1-f2cb-81e5bea1a110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[8. 2. 1. 4.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Softmax function"
      ],
      "metadata": {
        "id": "ukWRtPXJ2t1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hypothesis(x_data) :\n",
        "  softmax = tf.nn.softmax(tf.matmul(x_data, W) + b)\n",
        "  return softmax\n",
        "print(hypothesis(x_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-Ekhle0vWlV",
        "outputId": "a0ec7cbd-9255-44d7-8ab4-1f7a9262ecca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[2.6115654e-03 3.0526164e-04 9.9708307e-01]\n",
            " [5.1196496e-04 3.3310187e-08 9.9948794e-01]\n",
            " [1.9318848e-03 6.5735856e-10 9.9806809e-01]\n",
            " [3.8963979e-05 2.0478856e-14 9.9996096e-01]\n",
            " [9.1874064e-09 1.0007226e-12 9.9999994e-01]\n",
            " [3.7177328e-03 4.9338517e-10 9.9628228e-01]\n",
            " [6.6248433e-08 1.2685539e-13 9.9999982e-01]\n",
            " [5.0616271e-09 1.3120353e-15 9.9999994e-01]], shape=(8, 3), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cost Function"
      ],
      "metadata": {
        "id": "EKrrOqBD2EhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_fn(X, Y):\n",
        " logits = hypothesis(X)\n",
        " cost = -tf.reduce_sum(Y * tf.math.log(logits), axis=1)\n",
        " # print(cost)\n",
        " cost_mean = tf.reduce_mean(cost)\n",
        " return cost_mean\n",
        "\n",
        "print(cost_fn(x_data, y_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nx5f7sVv4UA",
        "outputId": "a1126897-9052-4522-a97a-9da36319eadf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(14.527027, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Function"
      ],
      "metadata": {
        "id": "RPdNPJzb2A9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_fn(X, Y):\n",
        " with tf.GradientTape() as tape:\n",
        "  cost = cost_fn(X, Y)\n",
        "  grads = tape.gradient(cost, variables)\n",
        "  return grads\n",
        "\n",
        "print(grad_fn(x_data, y_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dejYVEcox_Kt",
        "outputId": "14a28201-0ffd-45cd-fe3a-04056af4ae70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
            "array([[-0.24833688, -0.7499618 ,  0.9982986 ],\n",
            "       [-1.6231072 , -1.2499237 ,  2.8730307 ],\n",
            "       [-1.6214089 , -1.8749617 ,  3.4963706 ],\n",
            "       [-1.6207669 , -1.9999617 ,  3.6207283 ]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.24889846, -0.37496182,  0.6238602 ], dtype=float32)>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train"
      ],
      "metadata": {
        "id": "bWWSxAyD18iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(X, Y, epochs=2000, verbose=100):\n",
        " optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "\n",
        " for i in range(epochs):\n",
        "  grads = grad_fn(X, Y)\n",
        "  optimizer.apply_gradients(zip(grads, variables))\n",
        "\n",
        "  if (i==0) | ((i+1)%verbose==0):\n",
        "    print('Loss at epoch %.3d: %f' %(i+1, cost_fn(X,Y).numpy()))\n",
        "\n",
        "fit(x_data, y_data, 2000, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28ikblDRzkTE",
        "outputId": "f650a42d-5a93-4a7c-8d28-b34665c649ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at epoch 001: 0.123501\n",
            "Loss at epoch 100: 0.120158\n",
            "Loss at epoch 200: 0.116953\n",
            "Loss at epoch 300: 0.113910\n",
            "Loss at epoch 400: 0.111016\n",
            "Loss at epoch 500: 0.108261\n",
            "Loss at epoch 600: 0.105636\n",
            "Loss at epoch 700: 0.103132\n",
            "Loss at epoch 800: 0.100740\n",
            "Loss at epoch 900: 0.098453\n",
            "Loss at epoch 1000: 0.096265\n",
            "Loss at epoch 1100: 0.094170\n",
            "Loss at epoch 1200: 0.092162\n",
            "Loss at epoch 1300: 0.090235\n",
            "Loss at epoch 1400: 0.088386\n",
            "Loss at epoch 1500: 0.086608\n",
            "Loss at epoch 1600: 0.084899\n",
            "Loss at epoch 1700: 0.083255\n",
            "Loss at epoch 1800: 0.081672\n",
            "Loss at epoch 1900: 0.080146\n",
            "Loss at epoch 2000: 0.078675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prediction"
      ],
      "metadata": {
        "id": "3BGB7iSp13xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = hypothesis(x_data)\n",
        "print(a)\n",
        "print(tf.argmax(a, 1))\n",
        "print(tf.argmax(y_data, 1)) # matches with y_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfnfjXuw1g1g",
        "outputId": "aaa04192-5602-4807-b880-6e5435f04858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[3.6177348e-09 3.2240230e-05 9.9996775e-01]\n",
            " [3.5292879e-04 3.9862838e-02 9.5978415e-01]\n",
            " [1.5173010e-10 8.2513489e-02 9.1748655e-01]\n",
            " [6.3073109e-08 9.2534161e-01 7.4658245e-02]\n",
            " [1.3789122e-01 8.5634929e-01 5.7594753e-03]\n",
            " [7.3013373e-02 9.2698663e-01 4.1985544e-09]\n",
            " [8.4432364e-01 1.5567626e-01 1.4789474e-07]\n",
            " [9.7579849e-01 2.4201471e-02 2.8374919e-10]], shape=(8, 3), dtype=float32)\n",
            "tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n",
            "tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n"
          ]
        }
      ]
    }
  ]
}